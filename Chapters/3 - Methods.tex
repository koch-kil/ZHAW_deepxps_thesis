% Indicate the main file. Must go at the beginning of the file.
% !TEX root = ../main.tex

%----------------------------------------------------------------------------------------
% CHAPTER TEMPLATE
%----------------------------------------------------------------------------------------
\chapter{Methods} % Main chapter title

\label{Chapter3} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

\input{Chapters/3_1 Train_Data}


\input{Chapters/3_2 Test_Data}


\section{Classification types, model architectures \& training}
In this section, the classification types from our tasks, the models used and the training procedure used in this work are explained.
The tasks presented in the \nameref{Chapter1} are considered and assigned a task type.
% multi-class classification problem (exclusive)

The first task is a multi-class classification problem, which means we need to find one corresponding element for one of the two layers, where the elements are exclusive (only one is true). Task two seeks to infer the depth of the layer as a continuous number between 0 and 1. In our two-layer system, this is a regression problem, since there is a regressive behaviour between the input spectrum and the overlayer thickness. 
The third task at first seems to be multi-label classification problem, as each input can be assigned multiple true labels. However, it is a special type of multi-label classification, as the labels are not exclusively 0 or 1 but can take continuous values in between. This is because the proportional quantity of the respective elements is encoded in the label. 


To start the modelling procedure, the training dataset was split into training and validation datasets with a ration of 80:20. A first model was built respectively for each of the tasks listed in the \nameref{Chapter1} for the four models: CNN, CNN-DCT, CBAM, ViT. The models architectures were then changed such that it efficiently trains on the respective training dataset. This was evaluated by comparing training loss and accuracy.
The validation loss is then used to tune the hyperparameters and add normalization to the model until the model showed convex training behaviour and no overfitting. Lastly, the models were trained with an early stopping callback function which monitored the validation loss to stop the training when there was no improvement >0.1 for seven consecutive epochs. This ensures that the models don't overfit on the training data.

\subsection{Loss functions}
For the qualitative determination of top and bot layer elements, the categorical cross-entropy loss was used (see equation \ref{cce}).
The task of quantitative analysis was previously tackled and they described the categorical cross-entropy to \begin{quote}
    ... produce[s] unsatisfying results from a physics perspective, because it cannot deal well
with samples with several roughly equally present elements \cite{drera_deep_2019}.
\end{quote}
They solved this problem by implementing a custom loss function which includes the euclidean norm and multiplied with the squared predicted values as shown in equation \ref{eq:lossfn}. This loss function works well on this specific task and thus was used for the training of quantitative analysis.

\begin{equation}
\label{eq:lossfn}
    L(y, \hat{y}) = \sum(y_{i}^2 (y_{i} - \hat{y_{i}})^2
\end{equation}




\subsection{CNN}
% explain the structure
The CNN model used is based on the standard architecture with two convolutional blocks. The first blocks consist of two consecutive convolutional layers with increasing kernel sizes (3, 7) to extract more and more general features, while the second block has kernel sizes (17, 27, 47). The first block uses 32 filters and the second block uses 16 filters, both use the leaky-ReLU activation function, start with a batch-normalization layer and end with a max-pooling layer.
As these two blocks are used as a feature-extraction tool, the features are then flattened to one dimension before being connected through a dropout-layer (p = 0.4) and subsequent batch-normalization. Finally, the softmax function is applied to the dense output layer with 81 nodes corresponding to the elements.

During the development of an adequate CNN-model, batch-normalization seemed to have a big impact on the model and the training speed. A variety of kernel sizes in a variety of sequences were tested. Further, as CNN-models get bigger, a lot more parameters are trained, and thus, they get much more difficult to train and they take more time to train.


\subsection{CNN-DCT}
% explain the structure
The discrete cosine transform (DCT) of a natural signal decomposes it into the frequency domain, where the intensities correspond to cosine waves' intensity making up our original signal. Thus, it transforms a signal into a sparse representation, which means that the signal is composed of only a few important components. Although we would need all components to inversely reconstruct our signal, omitting some low-intensity but high frequency components from the DCT-transformed signal does not drastically change our signal. These so-called high frequency components often make up the noise of our signal. An example of a DCT-transformed XPS-signal is shown in Figure \ref{fig:dct}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\textwidth]{Figures/dct.png}
    \caption{Original simulated XPS-signal (a) and a sparse representation, the discrete-cosine transformed XPS-signal (b)}
    \label{fig:dct}
\end{figure}

This transformation gives our model a relation of signal intensity and peak position. The DCT type 2 transform is shown in equation \ref{eqn:dct}.

\begin{equation}
\label{eqn:dct}
X_k = \sum_{n=0}^{N-1} x_n \cdot \cos\left(\frac{\pi}{N}\left(n + \frac{1}{2}\right)k\right), \quad k = 0, 1, \ldots, N-1
\end{equation}

% show the structure
The model used for training has a parallel architecture, where the input is forked into two main blocks. One main block starts with the DCT transform. Afterwards, a convolutional block with increasing kernel sizes (3, 5, 7, 21) and 8 filters, which both end with a max-pooling layer with kernel size 2 are used. Finally, the features extracted from the convolutional blocks are flattened.
The other main block uses a first block of convolutional layers with kernel sizes (3, 7) and 32 filters and a second block with kernel sizes (17, 27, 47) with 16 filters, and both are followed by a max-pooling layer and before flattening. The flattened vectors are then concatenated and end in an multilayer perceptron tail with decreasing nodes (1024, 512) with dropout layers (p = 0.2) in between. Finally, a dense layer with 81 nodes corresponding to the elements forms the output. All convolutional layers use the leaky ReLU activation function and are followed by a batch-normalization layer for regularization.


\subsection{CBAM}
The code for the convolutional block attention module base blocks was found on Github and modified to fit our 1D-data \cite{mazzia__2023}. This was done by exchanging the 2D-version of pooling layers with the 1D-version in the channel attention. This is because as we apply convolutions on our 1D-signal, we get 2D-Input feature compared to a 3D-Input feature if using images as an input. Additionally, the spatial attention block was changed by applying 1D-convolutions on the concatenated average and max-pooled feature vectors. Further, the axes on which the average and max-pooling is computed was changed (from 3 to 2).
% explain the structure
The blocks were then included in a convolutional model with residual patterns, such that the CBAM blocks can learn on the extracted features with its attention-modules. 

% show the structure
To be specific, the model applies a convolution on the input before applying batch normalization and the ReLU activation function, followed by one CBAM block. After, a predefined number of residual blocks are appended consisting of three convolutional layers with increasing kernel sizes.
After the last block, it ends in a global average pooling layer which is connected to a dropout layer (p=0.2) and finally ends in our dense output layer.

\subsection{ViT}
The vision transformer model used for the 1D-Spectra was inspired from Yoni Gottesmans blogpost \cite{noauthor_interpretable_2023} who applied the model on electrocardiogram classification. 
It takes the published 2D-ViT model \cite{dosovitskiy_image_2021}as a basis and is adapted to work with one-dimensional inputs.
The model from the blogpost was then adapted to fit our input data shape with 1024 data points. Further, a patch-size of 4 data-points, a hidden size of 32. The Multilayer Perceptron (MLP) is of size 128 and the model consists of two transformer encoder blocks with a hidden size of 32. The hidden size  Additionally, a dropout layer with $p=0.1$ was added after and before the MLP, 
in multi head attention and after the embedded vector.
% explain the structure
With the structure chosen, an efficient training without substantial overfitting was achieved. 

% show the structure


\section{Model evaluation and accuracies}

When a model should be evaluated, the specific computation to obtain its performance must be adapted to its type, such as multi-class classification. 
To evaluate the models' performance and training process, the loss and accuracy was obtained and plotted. Furthermore, the test data obtained as described in Chapter \ref{test_data} was predicted and visualized in a confusion matrix.

For the third task of elemental quantification, a custom accuracy measure was introduced to be able to express the accuracy by means of a number and to show the comparability with similar methods. This function, shown in equation \ref{eq:threshacc}, only considers components with known relative share above 10\% ($\mathbb{I}(y_i > 0.1)$).It then computes the absolute difference of the true value ($y_{i}$) and the predicted value ($\hat{y}$). If this difference is more than 10\% of the true value, it is considered false, otherwise true. Thus, we evaluate what percentage of components our model predicts correctly within a 10\% margin, given the component contributes to at least 10\% of the contents.

\begin{equation}
\label{eq:threshacc}
\text{Threshold Accuracy} = \frac{\sum_{i=1}^{N} \mathbb{I}(y_i > 0.1) \cdot \mathbb{I}(|y_i - \hat{y}_i| < 0.1 \cdot y_i)}{\sum_{i=1}^{N} \mathbb{I}(y_i > 0.1)}
\end{equation}

