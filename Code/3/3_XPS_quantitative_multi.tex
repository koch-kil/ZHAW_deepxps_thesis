\hypertarget{imports}{%
\subsection*{Imports}\label{imports}}

\begin{lstlisting}[language=Python]
import sys
import json
import os
import gc
import glob
import pickle
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import tensorflow_addons as tfa
from tensorflow import keras
from tensorflow.keras import Model, layers
from sklearn.preprocessing import MultiLabelBinarizer
from numba import cuda

sys.path.append('../../modules') # add own modules
import preprocess, predict, functions_tf, base
\end{lstlisting}

\begin{lstlisting}[language=Python]
# Enable GPU memory growth
gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)

tf.random.set_seed(42)
\end{lstlisting}

\hypertarget{define-parameters}{%
\subsection*{Define Parameters}\label{define-parameters}}

\begin{lstlisting}[language=Python]
save_path = 'T:\\GItHub_Repos\\models\\3\\multi'
mlb, elements = base.retreive_mlb_and_elements()
n_elements = len(elements)
\end{lstlisting}

\hypertarget{load-dataset}{%
\subsection*{Load dataset}\label{load-dataset}}

\begin{lstlisting}[language=Python]
with open('../../data/training_data/3/dataset_multi.pkl', 'rb') as f:
    x = pickle.load(f)
\end{lstlisting}


\begin{lstlisting}[language=Python]
print(x['name'])
x_train = x['x_train']
y_train = x['y_train']
x_test = x['x_test']
y_test = x['y_test']
\end{lstlisting}

\begin{lstlisting}
mixed systems, one layer
\end{lstlisting}

\hypertarget{create-model}{%
\section*{Create model}\label{create-model}}

\hypertarget{cnn-model}{%
\subsection*{CNN model}\label{cnn-model}}

\begin{lstlisting}[language=Python]
from keras.layers import BatchNormalization, Dropout, Conv1D

name = 'cnn_16F_3_7_11_21_17_27_47_BN_Custom_Accuracy'

n_filters = 16

inputs = keras.Input(shape=(1,1024))
x_1 = layers.Reshape((1,1024))(inputs)
x_1 = layers.Dense(1024, activation='relu')(x_1)
x_1 = BatchNormalization()(x_1)
x_1 = Conv1D(filters=n_filters, kernel_size=3,  activation='leaky_relu', data_format='channels_first')(x_1)
x_1 = BatchNormalization()(x_1)
x_1 = Conv1D(filters=n_filters, kernel_size=7,   activation='leaky_relu', data_format='channels_first')(x_1)
x_1 = BatchNormalization()(x_1)
x_1 = Conv1D(filters=n_filters, kernel_size=11, activation='leaky_relu', data_format='channels_first')(x_1)
x_1 = BatchNormalization()(x_1)
x_1 = Conv1D(filters=n_filters, kernel_size=21,  activation='leaky_relu', data_format='channels_first')(x_1)
x_1 = layers.MaxPooling1D(2)(x_1)
x_1 = BatchNormalization()(x_1)

x_1 = Conv1D(filters=n_filters, kernel_size=17,   activation='leaky_relu', data_format='channels_first')(x_1)
x_1 = BatchNormalization()(x_1)
x_1 = Conv1D(filters=n_filters, kernel_size=27,  activation='leaky_relu', data_format='channels_first')(x_1)
x_1 = BatchNormalization()(x_1)
x_1 = Conv1D(filters=n_filters, kernel_size=47,  activation='leaky_relu', data_format='channels_first')(x_1)
x_1 = layers.MaxPooling1D(2)(x_1)
x_1 = layers.Flatten()(x_1)
x_1 = Dropout(0.2)(x_1)

x_1 = BatchNormalization()(x_1)
x_1 = layers.Dense(1024, activation='relu')(x_1)
x_1 = BatchNormalization()(x_1)
x_1 = Dropout(0.2)(x_1)
x_1 = layers.Dense(512, activation='relu', name='elements')(x_1)
x_1 = BatchNormalization()(x_1)
x_1 = Dropout(0.2)(x_1)

x_1 = layers.Dense(256, activation='relu')(x_1)

output_elements = layers.Dense(n_elements, activation='relu')(x_1)
output_elements = layers.Softmax(axis=-1)(output_elements)
output_elements = layers.Reshape((1,n_elements))(output_elements)

model = keras.Model(inputs=inputs, outputs=output_elements, name=name)
\end{lstlisting}

\begin{lstlisting}[language=Python]
batch_size = 2048

callback = tf.keras.callbacks.EarlyStopping(monitor='loss',
                                            min_delta=0.001,
                                            patience=10,
                                            restore_best_weights=True)

model.compile(
    optimizer= keras.optimizers.Adam(learning_rate=0.0002),
    loss= functions_tf.custom_loss,
    metrics = [functions_tf.custom_accuracy, tf.keras.metrics.MeanSquaredError()])

device = cuda.get_current_device()

history = model.fit(
    x_train.reshape(x_train.shape[0], 1, 1024),
    y_train.reshape(y_train.shape[0], 1, n_elements),
    batch_size = batch_size,
    verbose = 1,
    epochs = 250,
    shuffle=True,
    callbacks=[callback],
    validation_data = (x_test.reshape(x_test.shape[0], 1, 1024), 
                       y_test.reshape(y_test.shape[0], 1, n_elements))
)

gc.collect()

functions_tf.plot_and_save_history(name, history, model, 
                                   save_path,
                                   subfolder='CNN',
                                   plot_acc=False)
# del name
\end{lstlisting}

\hypertarget{cnn-dct}{%
\subsection*{CNN-DCT}\label{cnn-dct}}

\begin{lstlisting}[language=Python]
import importlib
importlib.reload(functions_tf)
import functions_tf
\end{lstlisting}

\begin{lstlisting}[language=Python]
# latest Model 27/06/2023
## parallel with dct transform
from keras.layers import BatchNormalization, Dropout, Conv1D, Dense, Reshape
name = 'cnn_dct_mae_32F_custom_loss'
filter_size = 32
x   = Dense(1024)(inputs)
x_1 = Reshape((1,1024))(x)
x_1 = BatchNormalization()(x)
x_1 = Conv1D(filters=filter_size, kernel_size=1, activation='leaky_relu',data_format='channels_first')(x_1)
x_1 = BatchNormalization()(x_1)
x_1 = Conv1D(filters=filter_size, kernel_size=3, activation='leaky_relu',data_format='channels_first')(x_1)
x_1 = BatchNormalization()(x_1)
x_1 = Conv1D(filters=filter_size, kernel_size=7, activation='leaky_relu',data_format='channels_first')(x_1)
x_1 = BatchNormalization()(x_1)
x_1 = layers.MaxPooling1D(2)(x_1)
x_1 = Conv1D(filters=filter_size, kernel_size=1, activation='leaky_relu',data_format='channels_first')(x_1)
x_1 = BatchNormalization()(x_1)
x_1 = Conv1D(filters=filter_size, kernel_size=3, activation='leaky_relu',data_format='channels_first')(x_1)
x_1 = BatchNormalization()(x_1)
x_1 = Conv1D(filters=filter_size, kernel_size=7, activation='leaky_relu',data_format='channels_first')(x_1)
x_1 = layers.MaxPooling1D(2)(x_1)
x_1 = layers.Flatten()(x_1)

x_2 = layers.Reshape((1,1024))(x)
x_2 = tf.signal.dct(x_2, name='dct_transform')
x_2 = Dense(1024, activation='relu',  kernel_regularizer='l1')(x_2)
x_2 = BatchNormalization()(x_2)
x_2 = Conv1D(filters=filter_size, kernel_size=3, activation='leaky_relu',data_format='channels_first')(x_2)
x_1 = BatchNormalization()(x_1)
x_2 = Conv1D(filters=filter_size, kernel_size=5, activation='leaky_relu',data_format='channels_first')(x_2)
x_1 = BatchNormalization()(x_1)
x_2 = Conv1D(filters=filter_size, kernel_size=7, activation='leaky_relu',data_format='channels_first')(x_2)
x_1 = BatchNormalization()(x_1)
x_2 = Conv1D(filters=filter_size, kernel_size=21, activation='leaky_relu',data_format='channels_first')(x_2)
x_2 = layers.MaxPooling1D(2)(x_2)
x_2 = BatchNormalization()(x_2)
x_2 = Conv1D(filters=filter_size, kernel_size=3, activation='leaky_relu',data_format='channels_first')(x_2)
x_1 = BatchNormalization()(x_1)
x_2 = Conv1D(filters=filter_size, kernel_size=5, activation='leaky_relu',data_format='channels_first')(x_2)
x_1 = BatchNormalization()(x_1)
x_2 = Conv1D(filters=filter_size, kernel_size=7, activation='leaky_relu',data_format='channels_first')(x_2)
x_1 = BatchNormalization()(x_1)
x_2 = Conv1D(filters=filter_size, kernel_size=21, activation='leaky_relu',data_format='channels_first')(x_2)
x_2 = layers.MaxPooling1D(2)(x_2)
x_2 = layers.Flatten(name='dct_features')(x_2)

x_1 = layers.Concatenate()([x_1, x_2])
# no learning from here on
x_1 = layers.Dense(1024, activation='relu')(x_1)
x_1 = Dropout(0.4)(x_1)
x_1 = BatchNormalization()(x_1)
elements_ = Dense(512, activation='relu', name='elements')(x_1)

elementstop = layers.Dense(256, activation='relu')(elements_)
elementstop = BatchNormalization()(elementstop)
elementstop = layers.Dropout(0.4)(elementstop)
elementstop = layers.Dense(n_elements, activation='leaky_relu')(elementstop)
elementstop = layers.Softmax(axis=-1)(elementstop)


model = keras.Model(inputs=inputs, outputs=elementstop, name="cnn_dct")

keras.utils.plot_model(model, f'{name}.png', show_layer_names=True, show_layer_activations=True, show_shapes=True)
\end{lstlisting}

\begin{lstlisting}[language=Python]
batch_size = 1024

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=10,
                                            min_delta=0.0001,
                                            restore_best_weights=True)

model.compile(
    optimizer= keras.optimizers.Adam(learning_rate=0.0002),
    loss= functions_tf.custom_loss, #reduction=tf.keras.losses.Reduction.SUM
    metrics = ['accuracy', 'mae', functions_tf.custom_accuracy])

device = cuda.get_current_device()

history = model.fit(
    x_train.reshape(x_train.shape[0], 1, 1024),
    y_train.reshape(y_train.shape[0], n_elements),
    batch_size = batch_size,
    verbose = 1,
    epochs = 250,
    shuffle=True,
    callbacks=[callback],
    validation_data = (x_test.reshape(x_test.shape[0], 1, 1024), y_test.reshape(y_test.shape[0], n_elements))
)

gc.collect()

functions_tf.plot_and_save_history(name, history, model, save_path, subfolder='DCT', plot_acc=False)
del name
\end{lstlisting}

\hypertarget{cbam}{%
\subsection*{CBAM}\label{cbam}}

\begin{lstlisting}[language=Python]
from functions_tf import build_1d_resnet_with_cbam

input_shape = (1, 1024)  # Adapted input shape
num_filters = 512 # Increase the number of filters in the CBAM block
model = build_1d_resnet_with_cbam(input_shape=input_shape, num_classes=n_elements, num_filters=num_filters, res_block_num=3, output_shape=(1,n_elements))
\end{lstlisting}

\begin{lstlisting}[language=Python]
name = 'CBAM_512_3_ES_MAE_350+EPOCHS'
subfolder = 'CBAM'
callback = tf.keras.callbacks.EarlyStopping(monitor='loss', 
                                            patience=20,
                                            min_delta=0.00008,
                                            restore_best_weights=True)

model.compile(optimizer= keras.optimizers.Adam(learning_rate=0.00005),
loss= functions_tf.custom_loss,
metrics = ['accuracy', functions_tf.custom_accuracy])


history = model.fit(
    x_train.reshape(x_train.shape[0], 1, 1024),
    y_train.reshape(y_train.shape[0],1,  n_elements),
    batch_size = 1024,
    verbose = 1,
    epochs = 350,
    shuffle=True,
    callbacks=[callback],
    validation_data = (x_test.reshape(x_test.shape[0], 1, 1024), y_test.reshape(y_test.shape[0],1,  n_elements))
    )


gc.collect()
keras.utils.plot_model(model, f'{name}.png', show_layer_names=True, show_layer_activations=True, show_shapes=True)
functions_tf.plot_and_save_history(name, history, model, save_path, subfolder=subfolder, plot_acc=False)
del name
\end{lstlisting}


\hypertarget{vision-transformer-model-vit}{%
\subsection*{Vision transformer model
(ViT)}\label{vision-transformer-model-vit}}

\begin{lstlisting}[language=Python]
from functions_tf import VisionTransformer

vit = VisionTransformer(
    patch_size=4,
    hidden_size=128,
    depth=3,
    num_heads=5,
    mlp_dim=512,
    num_classes=81,
    sd_survival_probability=1,
    dropout=0.1,
    attention_dropout=0.1,
    output_activation='softmax'
)
\end{lstlisting}

\begin{lstlisting}[language=Python]
optimizer = tf.keras.optimizers.Adam(0.001)
loss= functions_tf.custom_loss,
metrics = [tf.keras.metrics.MeanSquaredError(name='mse'), functions_tf.custom_accuracy]
vit.compile(optimizer=optimizer, loss=loss, metrics=metrics)

callback = tf.keras.callbacks.EarlyStopping(monitor='loss', min_delta=0.0001,
                                            patience=15,
                                            restore_best_weights=True)

history = vit.fit(
    x_train.reshape((x_train.shape[0],1024,1)),
    y_train.reshape(y_train.shape[0], 81),
    batch_size = 512,
    verbose = 1,
    epochs = 250,
    validation_data = (x_test.reshape((len(x_test),1024,1)),
                       y_test.reshape(y_test.shape[0], 81)),
    callbacks=[callback]
    )

name = 'vit_4_128_3_5_512_Custom_Loss'
vit.save_weights(f'{save_path}\\{subfolder}\\{name}_weights.h5')
pickle.dump(history.history, open(f'{save_path}\\plots_data\\history_{name}.pkl', 'wb'))
del name
\end{lstlisting}ยง