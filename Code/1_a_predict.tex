\hypertarget{read-original-files}{%
\subsection{Read original files}\label{read-original-files}}

\begin{lstlisting}[language=Python]
predict_original_files('data/example_data/Filo_220718_116_4_Mg5.csv')
\end{lstlisting}

\begin{lstlisting}[language=Python]
df = pd.read_csv('data/example_data/clma001.csv', skiprows=7, sep='\t', names=['x', 'y', 'z', 'a'])
df = df[['x','y']]
f = len(df.x)
t = np.interp(x = np.flip(np.arange(0,f,step=f/2048)),xp=[x for x in range(f)], fp=df.y)
t = minMaxScaler(t)
prediction = model.predict(t.reshape(1,1,2048), batch_size=1)
p = prediction.reshape(2,13)
top = np.zeros(13)
bot = np.zeros(13)
top[p[0].argmax()] = int(1)
bot[p[1].argmax()] = int(1)
top = top.astype(int)
bot = bot.astype(int)
pred = np.array([top, bot])
pr = mlb.inverse_transform(pred)
print(f'Prediction: \t  \t {pr[1][0]} on {pr[0][0]}')
\end{lstlisting}

\begin{lstlisting}[language=Python]
df
\end{lstlisting}

\begin{lstlisting}[language=Python]
plt.plot(df.x, df.y)
t = np.interp(x = np.flip(np.arange(0,f,step=f/2048)),xp=[x for x in range(f)], fp=df.y)
plt.plot(t)
plt.xlim(2048,0)
f = len(df.x)
\end{lstlisting}

\begin{lstlisting}[language=Python]
df.x
\end{lstlisting}

\begin{lstlisting}[language=Python]
predict_original_files('data/example_data/clma001.csv')
\end{lstlisting}

\begin{lstlisting}[language=Python]
df = pd.read_csv('data/example_data/clma001.csv', skiprows=7, sep='\t', names=['x', 'y', 'z', 'a'])
\end{lstlisting}

\begin{lstlisting}[language=Python]
plt.plot(df.x, df.y)
plt.xlim(1468.6,0)
# This is carbon with s1 peak at 281, not trained yet
\end{lstlisting}

\begin{lstlisting}[language=Python]
df = pd.read_csv('data/example_data/clma001.csv', skiprows=7, sep='\t', names=['x', 'y', 'z', 'a'])
\end{lstlisting}

\begin{lstlisting}[language=Python]
df
\end{lstlisting}

\begin{lstlisting}[language=Python]
plt.plot(df.x, df.y)
plt.xlim(1468.6,0)
df.x[df.y.argmax()]
\end{lstlisting}

\begin{lstlisting}[language=Python]
import scipy
indx = scipy.signal.find_peaks(df.y, prominence=1000)[0] # Oxygen and Rhodium / Magnesium
df.x[indx]
\end{lstlisting}

\begin{lstlisting}[language=Python]
import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
path = r'C:\Users\kochk\Documents\Git_Repos\Github\deep_xps\data\simulation_data\depth'
files = os.listdir(path)
\end{lstlisting}

\begin{lstlisting}[language=Python]
for file in files[:180]:
    df = pd.read_csv(path+'\\'+file, skiprows=1, header=None, sep='\s+', names=['x', 'y'])
    plt.plot(df.y)
\end{lstlisting}

\begin{lstlisting}[language=Python]
file
\end{lstlisting}

\begin{lstlisting}[language=Python]
f = len(df.x)
t = np.interp(x = np.flip(np.arange(0,f,step=f/2048)),xp=[x for x in range(f)], fp=df.y)
t = minMaxScaler(t)
\end{lstlisting}

\begin{lstlisting}[language=Python]
import matplotlib.pyplot as plt
plt.plot(df.x, df.y)
t = np.interp(x = np.flip(np.arange(0,f,step=f/2048)),xp=[x for x in range(f)], fp=df.y)
#plt.plot(t)
f = len(df.x)
\end{lstlisting}

\hypertarget{predict}{%
\subsection{Predict}\label{predict}}

\hypertarget{random-stuff-to-see-the-predictions}{%
\subsubsection{Random stuff to see the
predictions}\label{random-stuff-to-see-the-predictions}}

\begin{lstlisting}[language=Python]
import random
import os
import pandas as pd
labs = pd.DataFrame(mlb.fit_transform([elements]), columns=mlb.classes_)
fig, ax = plt.subplots(2,2, figsize=(20,8));
plt.tight_layout();

gradient = random.choice(['etching', 'separate'])
if gradient == 'etching':
    EXPERIMENT_DIR = 'depth_CO'
else:
    EXPERIMENT_DIR = 'depth_CO_sep'

exp_files = os.listdir(f'./data/simulation_data/{EXPERIMENT_DIR}/')
testfile = random.choice(exp_files)

test_params = testfile.split('.')[0].split('_')[:3]
print(test_params)
test_params = ['Be', 'B', 12]
true_string = f'True: {test_params[0]}, {test_params[1]}, {gradient}, {test_params[2]}'
print(true_string)

file_path =f'data/simulation_data/{EXPERIMENT_DIR}/{test_params[0]}_{test_params[1]}_{test_params[2]}_{gradient}_spectra.spcreg1.spc'
test  = pd.read_csv(file_path, sep='\s+')
ax[0,0].plot(test['#energy'].to_numpy(), test['geo_1'].to_numpy());
ax[0,0].set_xlim( min(test['#energy'].to_numpy()), max(test['#energy'].to_numpy()))
data = test['geo_1'].to_numpy().reshape(1, 1,2048)
print(f'Predicting file {test_params[0]}_{test_params[1]}_{test_params[2]}_{gradient}_spectra.spcreg1.spc')
prediction = model.predict(minMaxScaler(data), batch_size=1, verbose=True)
labels_encoded = np.array([mlb.transform([[test_params[0]]])[0], mlb.transform([[test_params[1]]])[0]]).reshape(2,79)
ax[0,1].imshow(labels_encoded)
ax[0,1].title.set_text(true_string)
ax[0,1].set_xticks([i for i in range(len(elements))], labels=labs.columns.values, fontdict={'fontsize': 8}); 
ax[0,1].set_yticks([0,1], labels=['top', 'bot']); 

predictions = np.array([prediction[0], prediction[1]]).reshape(2,79)

pred_string  = 'Prediction: '

for layer in predictions:
    new_layer = np.zeros(shape=(1,79))
    new_layer[0][layer.argmax()] = 1
    pred_string += str(mlb.inverse_transform(new_layer)[0][0]) + ', '
print(pred_string)
ax[1,1].imshow(predictions)
ax[1,1].title.set_text(pred_string)
ax[1,1].set_xticks([i for i in range(len(elements))], labels=labs.columns.values, fontdict={'fontsize': 8});
ax[1,1].set_yticks([0,1], labels=['top', 'bot']); 
ax[1,0].plot(prediction[0][0][0])
ax[1,0].plot(prediction[1][0][0])
ax[1,0].scatter(prediction[0][0][0].argmax(), 1, facecolors='none', edgecolors='r')
ax[1,0].scatter(prediction[1][0][0].argmax(), 1, facecolors='none', edgecolors='g')
ax[1,0].plot(np.array(labels_encoded, np.float32).reshape(2,79)[0], 'rx')
ax[1,0].plot(np.array(labels_encoded, np.float32).reshape(2,79)[1], 'gx')
ax[1,0].set_ylim([0.01,1.1])
ax[1,0].legend(['pred_layer 1', 'pred_layer 2', 'argmax_layer 1', 'argmax_layer 2', 'true_layer 1', 'true_layer 2'])
acc = tf.keras.metrics.CategoricalAccuracy()
acc.update_state(np.array(labels_encoded, np.float32), predictions)
print(f'Accuracy: {acc.result().numpy()}')
# plt.savefig('Wrong softmax axis.png')
\end{lstlisting}

\begin{lstlisting}
---------------------------------------------------------------------------

FileNotFoundError                         Traceback (most recent call last)

Cell In[66], line 14

     11 else:

     12     EXPERIMENT_DIR = 'depth_CO_sep'

---> 14 exp_files = os.listdir(f'./data/simulation_data/{EXPERIMENT_DIR}/')

     15 testfile = random.choice(exp_files)

     17 test_params = testfile.split('.')[0].split('_')[:3]



FileNotFoundError: [WinError 3] The system cannot find the path specified: './data/simulation_data/depth_CO/'
\end{lstlisting}

\includegraphics{ce1bba8aef2fd64bc891c2dd8f3ef507a77c7c05.png}

\begin{lstlisting}[language=Python]
plt.plot(prediction[0][0][0])
\end{lstlisting}

\begin{lstlisting}
[<matplotlib.lines.Line2D at 0x173c2e6d0a0>]
\end{lstlisting}

\includegraphics{3d8b295bd5d80560bd0a6fa095bae9ce78866ebd.png}

\begin{lstlisting}[language=Python]
def predict_one_example(model, random:bool=True, params=None):
    
    import random

    fig, ax = plt.subplots(1,2, figsize=(12,5))
    plt.grid(True)
    
    etching = random.randint(0,20)
    test_set = random.sample(elements, 2)
    test_params = [test_set[0], test_set[1], etching]
    
    if params !=None:
        test_params = params
    test  = pd.read_csv(f'data/simulation_data/{EXPERIMENT_DIR}/{test_params[0]}_{test_params[1]}_{test_params[2]}_etching_spectra.spcreg1.spc', sep='\s+')
    ax[0].plot(minMaxScaler(test['geo_1'].to_numpy()))
    data = minMaxScaler(test['geo_1'].to_numpy().reshape(1, 1,2048))
    prediction = model.predict(data, batch_size=1)
    p = prediction.reshape(output_shape[0], output_shape[1])


    fname = files[counter]
    frame = pd.read_csv(INPUT_DATA_DIR + fname, skiprows=1, header=None, sep='\s+')

    # preprocessing
    'etching' in fname.split('.')[0] # means we have a gradient!
    top, bottom, etching = fname.split('.')[0].split('_')[:3] 
    # gives top, bottom, etching-depth, where top starts at 100% (with 0 etching) on the top
    etching = int(etching)

    encoded = np.zeros(output_shape)
    for layer in range(output_shape[0]):
        if etching > layer:
            continue
        else:
            encoded[layer] = np.array(mlb.transform([[top]])*rev[layer]+mlb.transform([[bottom]])*gradient[layer])

    tr = mlb.transform([[test_params[0]], [test_params[1]]])
    ax[1].plot(p[0]);
    ax[1].plot(p[1]);
    ax[1].plot(tr[0], 'g+');
    ax[1].plot(tr[1], 'r+');
    ax[1].legend(['predicted_bot', 'predicted_top', 'true_bot', 'true_top'])
    ax[1].set_xticks([i for i in range(len(elements))], labels=labs.columns.values)
#    ax[1].set_xticklabels(labs.columns.values)
    top = np.zeros(13)
    bot = np.zeros(13)
    top[p[0].argmax()] = int(1)
    bot[p[1].argmax()] = int(1)
    top = top.astype(int)
    bot = bot.astype(int)
    pred = np.array([top, bot])
    pr = mlb.inverse_transform(pred)
    print(f'Correct solution: \t {test_params[1]} on {test_params[0]}')
    print(f'Prediction: \t  \t {pr[1][0]} on {pr[0][0]}')
    print(test_params[3])
    m = tf.keras.metrics.CategoricalAccuracy()
    m.update_state(tr, pred)
    
    print(f'Accuracy: {m.result().numpy()}')
\end{lstlisting}

\begin{lstlisting}[language=Python]
def eval_one_example_model(model, random:bool=True, params=None, n=1):
    from typing import List
    # bottom layer - top layer - bottom thickness - top thickness
    import random
    if n==1:
        if params !=None:
            test_params = params
        else:
            test_set = random.sample(elements, 2)
            test_params = [test_set[0], test_set[1], 50, 15]
        test  = pd.read_csv(f'data/simulation_data/{EXPERIMENT_DIR}/{test_params[0]}_{test_params[2]}_{test_params[1]}_{test_params[3]}_spectra.spcreg1.spc', sep='\s+')
        test = test['geo_1'].to_numpy().reshape(1, 1,2048)
        dataset = minMaxScaler(test)
        tr = mlb.transform([[test_params[0]], [test_params[1]]])
        tr = tr.reshape(1,2,13)
    
    else: 
        for i in range(n):
            test_set = random.sample(elements, 2)
            test_params = [test_set[0], test_set[1], 50, 15]
            test  = pd.read_csv(f'data/simulation_data/{EXPERIMENT_DIR}/{test_params[0]}_{test_params[2]}_{test_params[1]}_{test_params[3]}_separate_spectra.spcreg1.spc', sep='\s+')
            test = test['geo_1'].to_numpy().reshape(1, 1,2048)
            data = minMaxScaler(test)
            tr = mlb.transform([[test_params[0]], [test_params[1]]])

            if i ==0:
                dataset = data
                labels = tr
            else:
                dataset = np.append(dataset, data, axis=0)
                labels = np.append(labels, tr, axis=0)
        dataset = dataset.reshape(n,1,2048)
        labels = labels.reshape(n,2,13)
        
    prediction = model.evaluate(dataset, labels, batch_size=n)
    #print(f'Correct solution: \t {test_params[1]} on {test_params[0]}')
    #print(test_params[3])
\end{lstlisting}

\begin{lstlisting}[language=Python]
eval_one_example_model(model_p, n=250)
\end{lstlisting}

\begin{lstlisting}[language=Python]
predict_one_example(model=model, params=['Si', 'Mg',50,15])
\end{lstlisting}

\hypertarget{visualizations}{%
\subsection{Visualizations}\label{visualizations}}

\begin{lstlisting}[language=Python]
df = pd.read_pickle('../../data/df_6.pkl')
plt.figure(figsize=(15,7))
plt.plot(x_new, df['Fe_Fe'][::2].iloc[:,0].values, alpha=0.7)
plt.plot(x_new, df['Fe_Fe'][::2].iloc[:,1].values, alpha=0.7)
plt.plot(x_new, np.flip(y_new), alpha=1)
plt.xlim(1024,0)
plt.legend(['simulated, with adv. C/O Layer',
            'simulated, without adv. C/O Layer',
            'experimental, ion-etched'])
plt.xlabel('Binding Energy [eV]')
plt.ylabel('Intensity [a.u.]')
plt.title('XPS Spectra of Iron (Fe)');
plt.text(900, 0.8, 'Fe Auger-Peak', fontsize=12)
plt.savefig('../../data/../documentation/figures/Fe_XPS.png', dpi=300)
\end{lstlisting}

\begin{lstlisting}[language=Python]
plt.figure(figsize=(20,7))
plt.plot(x_new, df_scaled.T['Fe_Fe'].iloc[:,0].values, alpha=0.4)
plt.plot(x_new, df_norm['Fe_Fe'][::2].iloc[:,1].values, alpha=0.4)
plt.plot(x_new, np.flip(y_new), alpha=1)
plt.xlim(800,600)
plt.legend(['simulated, with adv. C/O Layer', 'simulated, without adv. C/O Layer', 'experimental, ion-etched'])
plt.xlabel('Binding Energy [eV]')
plt.ylabel('Intensity [a.u.]')
plt.title('XPS Spectra of Iron (Fe)');
plt.savefig('../../data/../documentation/figures/Fe_Peak_XPS.png', dpi=300)
\end{lstlisting}

\includegraphics{230ecbf6b55e252d900d293cbe5830f73a712b85.png}

\hypertarget{load-models-and-predict-the-dataset}{%
\subsection{Load models and predict the
dataset}\label{load-models-and-predict-the-dataset}}

\begin{lstlisting}[language=Python]
import sys
sys.path.append('../../../modules/')
import os
import gc
import base
import json
import pickle
import predict
import numpy as np
import pandas as pd
import functions_tf
import tensorflow as tf
import matplotlib.pyplot as plt
import tensorflow.keras as keras
from sklearn.preprocessing import MultiLabelBinarizer
from functions_tf import ChannelAttention, SpatialAttention

mlb, elements = base.retreive_mlb_and_elements()
n_elements = len(elements)
df = pd.read_pickle(r'C:\Users\kochk\Documents\Git_Repos\Github\deep_xps\data\experimental_data_elemental.pkl')
\end{lstlisting}

\begin{lstlisting}[language=Python]
x_exp, y_exp = [df.T.values, df.columns.map(lambda x: x.split('_')[1]).values] # top layer

modeldir = r"T:\GItHub_Repos\models\1\mixcont\top_layer\models"
PLOTSDIR= r"T:\GItHub_Repos\models\1\mixcont\top_layer\models\plots_data"
KEYWORD = 'CBAM'

for root, folder, filename in os.walk(modeldir):
    for file in filename:
        if file.endswith('.h5') and KEYWORD in root and not 'vit' in file:
            MODELPATH = os.path.join(root, file)
            print(f'{MODELPATH}')
            print(file)
            if 'CBAM' in MODELPATH:
                model = keras.models.load_model(MODELPATH,
                                                custom_objects={'ChannelAttention': ChannelAttention,
                                                                           'SpatialAttention': SpatialAttention,
                                                                           'CBAM': functions_tf.CBAM})
            else:
                model = keras.models.load_model(MODELPATH)
            predictions = predict.predict_from_array_h5(x_exp, y_exp,
                                                        shape=(x_exp.shape[0], 1,1024),
                                                        model=model)
            acc = np.array([i[0] for i in predictions]).sum() / len(predictions)
            trainable_vars_N = np.sum([np.prod(v.shape) for v in model.trainable_variables])
            print(f'Number of trainable variables: {trainable_vars_N}')
            historyfile = os.path.join(PLOTSDIR, str('history_'+file.split('.')[0]+'.pkl'))
            if os.path.exists(historyfile):
                with open(historyfile, 'rb') as f:
                    history = pickle.load(f)
                print('Training set: \t', round(history['categorical_accuracy'][-1]*100, 2), '%')
                print('Valid. set: \t',round(history['val_categorical_accuracy'][-1]*100, 2), '%')
            print('Test set: \t',round(acc*100, 2), '%')
            print('\n')
            gc.collect()
            
\end{lstlisting}

\begin{lstlisting}
T:\GItHub_Repos\models\1\mixcont\top_layer\models\CBAM\CBAM_2Blocks_512F_bs2048.h5
CBAM_2Blocks_512F_bs2048.h5
Number of trainable variables: 20077275
Training set: 	 92.8 %
Valid. set: 	 82.8 %
Test set: 	 30.23 %


T:\GItHub_Repos\models\1\mixcont\top_layer\models\CBAM\CBAM_3Blocks_128F_bs2048.h5
CBAM_3Blocks_128F_bs2048.h5
Number of trainable variables: 2464809
Training set: 	 75.74 %
Valid. set: 	 75.37 %
Test set: 	 40.93 %


\end{lstlisting}

\hypertarget{single-prediction}{%
\subsection{Single prediction}\label{single-prediction}}

\begin{lstlisting}[language=Python]
model = keras.models.load_model(r"T:\GItHub_Repos\models\1\mixcont\top_layer\models\CBAM\CBAM_3Blocks_512F.h5",
                                custom_objects={'CBAM': functions_tf.CBAM})
x_exp, y_exp = [df.T.values, df.columns.map(lambda x: x.split('_')[0]).values] # top layer
pred = model.predict(x_exp.reshape(x_exp.shape[0], 1, 1024), batch_size=213)
truelabels = np.array([mlb.transform([[i]]) for i in y_exp])
cce = tf.keras.losses.CategoricalCrossentropy()
cce(truelabels, pred).numpy()
print(f'Loss: {cce(truelabels, pred).numpy()}')
print(f'Accuracy: {round(tf.keras.metrics.CategoricalAccuracy()(truelabels, pred).numpy()*100, 2)}')
\end{lstlisting}

\begin{lstlisting}[language=Python]
 historyfile = os.path.join(PLOTSDIR, str('history_CBAM_3Blocks_512F.pkl'))

 if os.path.exists(historyfile):
     with open(historyfile, 'rb') as f:
         history = pickle.load(f)
     print('Training set: \t', round(history['categorical_accuracy'][-1]*100, 2), '%')
     print('Valid. set: \t',round(history['val_categorical_accuracy'][-1]*100, 2), '%')
\end{lstlisting}

\begin{lstlisting}[language=Python]
np.sum([np.prod(v.shape) for v in model.trainable_variables])
\end{lstlisting}

\begin{lstlisting}
28241673
\end{lstlisting}

\hypertarget{vit}{%
\subsection{VIT}\label{vit}}

\begin{lstlisting}[language=Python]
x_exp, y_exp = [df.T.values, df.columns.map(lambda x: x.split('_')[0]).values] # top layer
PLOTSDIR= r"T:\GItHub_Repos\models\1\mixcont\top_layer\models\plots_data"
vitdir = r"T:\GItHub_Repos\models\1\mixcont\top_layer\models\VIT"
for file in os.listdir(vitdir):
    if 'vit' not in file or os.path.isdir(os.path.join(vitdir,file)):
        continue
    params = file.split('_')
    print(f'going for {file}')
    print(params)
    from functions_tf import VisionTransformer

    vit = VisionTransformer(
        patch_size=int(params[1]),
        hidden_size=int(params[2]),
        depth=int(params[3]),
        num_heads=int(params[4]),
        mlp_dim=int(params[5]),
        num_classes=81,
        sd_survival_probability=1,
        dropout=0.1,
        attention_dropout=0.1
    )
    vit.build(input_shape=(None, 1024, 1))
    vit.load_weights(os.path.join(vitdir,file))
    
    predictions = predict.predict_from_array(x_exp, y_exp, shape=(x_exp.shape[0], 1024,1), model=vit)
    acc = np.array([i[0] for i in predictions]).sum() / len(predictions)
    trainable_vars_N = np.sum([np.prod(v.shape) for v in vit.trainable_variables])
    print(f'Number of trainable variables: {trainable_vars_N}')
    historyfile = os.path.join(PLOTSDIR, str('history_'+file.split('.')[0][:-8]+'.pkl'))
    if os.path.exists(historyfile):
        with open(historyfile, 'rb') as f:
            history = pickle.load(f)
        print('Training set: \t', round(history['categorical_accuracy'][-1]*100, 2), '%')
        print('Valid. set: \t',round(history['val_categorical_accuracy'][-1]*100, 2), '%')
    print(f'Test set:\t {round(acc*100, 2)}%')
\end{lstlisting}

\begin{lstlisting}
going for vit_4_32_2_3_128_weights.h5
['vit', '4', '32', '2', '3', '128', 'weights.h5']
1/1 [==============================] - 0s 224ms/step
Number of trainable variables: 35793
Training set: 	 79.92 %
Valid. set: 	 82.69 %
Test set:	 52.56%
\end{lstlisting}
