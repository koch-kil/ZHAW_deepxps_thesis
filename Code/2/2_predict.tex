\hypertarget{load-models-and-predict-the-dataset}{%
\subsection*{Load models and predict the
dataset}\label{load-models-and-predict-the-dataset}}

\begin{lstlisting}[language=Python]
import sys
sys.path.append('../../modules/')
import os
import gc
import base
import json
import pickle
import predict
import numpy as np
import pandas as pd
import functions_tf
import tensorflow as tf
import matplotlib.pyplot as plt
import tensorflow.keras as keras
from sklearn.preprocessing import MultiLabelBinarizer
from functions_tf import ChannelAttention, SpatialAttention

mlb, elements = base.retreive_mlb_and_elements()
n_elements = 5
\end{lstlisting}

\begin{lstlisting}[language=Python]
df = pd.read_pickle('../../data/depth_profile.pkl')
top_layer_modeldir = r"T:\GItHub_Repos\models\2\depth"
\end{lstlisting}

\begin{lstlisting}[language=Python]
def angstrom_to_label(angstrom):
    if angstrom < 0:
        return 0.0
    elif angstrom >= 100:
        return 1.0
    else:
        mapped_value = angstrom / 100.0
        return min(mapped_value, 1.0)

def weighted_sum(predicted_probabilities):
    '''transform the predicted probabilities into a single value'''
    return sum(p * n for p, n in zip(predicted_probabilities, [1,2,3,4,5]))
\end{lstlisting}

\begin{lstlisting}[language=Python]
import sys
import os
sys.path.append('../../modules/')
from functions_tf import ChannelAttention, SpatialAttention, GlobalAveragePooling1D
import tensorflow.keras as keras
import gc
x_exp, y_exp = [df.T.values,
                df.columns.map(lambda x: x.split('_')[2]).map(int).map(angstrom_to_label)]

for root, folder, filename in os.walk(top_layer_modeldir):
    for file in filename:
        if file.endswith('.h5') and not 'vit' in file.lower():
            gc.collect()
            MODELPATH = os.path.join(root, file)
            print(file)
            if 'CBAM' in MODELPATH:
                model = keras.models.load_model(MODELPATH, custom_objects={'ChannelAttention': ChannelAttention, 'SpatialAttention': SpatialAttention, 'CBAM': functions_tf.CBAM})
            else:
                model = keras.models.load_model(MODELPATH)
            predictions = model.predict(x_exp.reshape(x_exp.shape[0], 1, 1024))
            pr = [(round(weighted_sum(i[0]), 2) , df.columns[k])for k, i in enumerate(predictions)]
            print(pr)
            trainable_vars_N = np.sum([np.prod(v.shape) for v in model.trainable_variables])
            print(f'Number of trainable variables: {trainable_vars_N}')
            historyfile = os.path.join(PLOTSDIR, str('history_'+file.split('.')[0]+'.pkl'))
            if os.path.exists(historyfile):
                with open(historyfile, 'rb') as f:
                    history = pickle.load(f)
                print('Training set: \t', round(history['categorical_accuracy'][-1]*100, 2), '%')
                print('Valid. set: \t',round(history['val_categorical_accuracy'][-1]*100, 2), '%')
\end{lstlisting}


\hypertarget{vit}{%
\subsubsection*{VIT}\label{vit}}

\begin{lstlisting}[language=Python]
PLOTSDIR= r"T:\GItHub_Repos\models\2\depth\plots_data"
vitdir = r"T:\GItHub_Repos\models\2\depth\VIT"

x_exp, y_exp = [df.T.values,
                df.columns.map(lambda x: x.split('_')[2]).map(int).map(angstrom_to_label)]

for file in os.listdir(vitdir):
    if 'vit' not in file:
        continue
    params = file.split('_')
    print(f'going for {file}')
    print(params)
    from functions_tf import VisionTransformer

    vit = VisionTransformer(
        patch_size=int(params[1]),
        hidden_size=int(params[2]),
        depth=int(params[3]),
        num_heads=int(params[4]),
        mlp_dim=int(params[5]),
        num_classes=5,
        sd_survival_probability=1,
        dropout=0.1,
        attention_dropout=0.1
    )
    vit.build(input_shape=(None, 1024, 1))
    vit.load_weights(os.path.join(vitdir,file))
    
    predictions = vit.predict(x_exp.reshape(x_exp.shape[0], 1024, 1))
    pr = [(round(weighted_sum(i), 2) , df.columns[k])for k, i in enumerate(predictions)]
    print(pr)
    trainable_vars_N = np.sum([np.prod(v.shape) for v in vit.trainable_variables])
    print(f'Number of trainable variables: {trainable_vars_N}')
    historyfile = os.path.join(PLOTSDIR, str('history_'+file.split('.')[0][:-8]+'.pkl'))
    if os.path.exists(historyfile):
        with open(historyfile, 'rb') as f:
            history = pickle.load(f)
        print('Training set: \t', round(history['categorical_accuracy'][-1]*100, 2), '%')
        print('Valid. set: \t',round(history['val_categorical_accuracy'][-1]*100, 2), '%')
    print(f'Test set:\t {round(acc*100, 2)}%')
\end{lstlisting}
