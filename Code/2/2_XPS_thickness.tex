\hypertarget{imports}{%
\subsection*{Imports}\label{imports}}

\begin{lstlisting}[language=Python]
import sys
import json
import gc
import glob
import pickle
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
import tensorflow_addons as tfa
from tensorflow import keras
from tensorflow.keras import Model, layers
from sklearn.preprocessing import MultiLabelBinarizer
from numba import cuda

sys.path.append('../../modules/') # add own modules
import preprocess, predict, functions_tf, base
\end{lstlisting}

\begin{lstlisting}[language=Python]
# Enable GPU memory growth
gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)
\end{lstlisting}

\begin{lstlisting}[language=Python]
tf.random.set_seed(42)
np.random.seed(42)
\end{lstlisting}

\hypertarget{define-parameters}{%
\subsection*{Define Parameters}\label{define-parameters}}

\begin{lstlisting}[language=Python]
save_path = 'T:\\GItHub_Repos\\models\\1\\mixcont\\bot_layer\\models'
mlb, elements = base.retreive_mlb_and_elements()
n_elements = len(elements)
\end{lstlisting}

\begin{lstlisting}[language=Python]
save_path = r'T:\GItHub_Repos\models\2\depth'
\end{lstlisting}

\hypertarget{load-dataset}{%
\subsection*{Load dataset}\label{load-dataset}}

\begin{lstlisting}[language=Python]
import pickle
with open('../../data/training_data/2/dataset_depth.pkl', 'rb') as f:
    x = pickle.load(f)
\end{lstlisting}

\begin{lstlisting}[language=Python]
print(x['name'])
x_train = x['x_train']
y_train = x['y_train']
x_test = x['x_test']
y_test = x['y_test']
\end{lstlisting}

\begin{lstlisting}
depth profile labels
\end{lstlisting}

\begin{lstlisting}[language=Python]
n_outputs = 1
y_test[4000] # thickness-class
\end{lstlisting}

\begin{lstlisting}
array([3])
\end{lstlisting}

\begin{lstlisting}[language=Python]
y_train = tf.one_hot(y_train, 5)
y_test = tf.one_hot(y_test, 5)
\end{lstlisting}

\begin{lstlisting}[language=Python]
n_outputs = 5
n_elements = 5
\end{lstlisting}

\hypertarget{create-model}{%
\section*{Create model}\label{create-model}}

\hypertarget{cnn-model}{%
\subsubsection*{CNN model}\label{cnn-model}}

\begin{lstlisting}[language=Python]
from keras.layers import BatchNormalization, Dropout, Conv1D

name = 'cnn_32F_3_7_17_27_47_LRELU_1024_BN'

n_filters = 32

inputs = keras.Input(shape=(1,1024))
x_1 = layers.Reshape((1,1024))(inputs)

x_1 = BatchNormalization()(x_1)
x_1 = Conv1D(filters=n_filters, kernel_size=3,  activation='leaky_relu', data_format='channels_first')(x_1)
x_1 = BatchNormalization()(x_1)
x_1 = Conv1D(filters=n_filters, kernel_size=7,  activation='leaky_relu', data_format='channels_first')(x_1)
x_1 = BatchNormalization()(x_1)
x_1 = layers.MaxPooling1D(2)(x_1)

x_1 = Conv1D(filters=n_filters/2, kernel_size=17, activation='leaky_relu', data_format='channels_first')(x_1)
x_1 = BatchNormalization()(x_1)
x_1 = Conv1D(filters=n_filters/2, kernel_size=27, activation='leaky_relu', data_format='channels_first')(x_1)
x_1 = BatchNormalization()(x_1)
x_1 = Conv1D(filters=n_filters/2, kernel_size=47, activation='leaky_relu', data_format='channels_first')(x_1)
x_1 = BatchNormalization()(x_1)
x_1 = layers.MaxPooling1D(2)(x_1)

x_1 = layers.Flatten()(x_1)
x_1 = Dropout(0.2)(x_1)
x_1 = layers.Dense(1024, activation='leaky_relu')(x_1)
x_1 = layers.Dense(512, activation='leaky_relu', name='elements')(x_1)
x_1 = BatchNormalization()(x_1)


output_elements = layers.Dense(n_elements, activation='leaky_relu')(x_1)
output_elements = layers.Softmax(axis=-1)(output_elements)
output_elements = layers.Reshape((1, n_elements), name='output1')(output_elements)

model = keras.Model(inputs=inputs, outputs=output_elements, name=name)
\end{lstlisting}

\begin{lstlisting}[language=Python]
batch_size = 2048

x_train = x_train.reshape(x_train.shape[0], 1, 1024)

callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10,
                                            min_delta=0.0001,
                                            restore_best_weights=True)

model.compile(
    optimizer= keras.optimizers.Adam(learning_rate=0.00001),
    loss= tf.keras.losses.CategoricalCrossentropy(),
    metrics = 'categorical_accuracy')

device = cuda.get_current_device()

history = model.fit(
    x_train,
    y_train,
    batch_size = batch_size,
    verbose = 1,
    epochs = 250,
    shuffle=True,
    callbacks=[callback],
    validation_data = (x_test.reshape(x_test.shape[0], 1, 1024), y_test)
    )

gc.collect()
functions_tf.plot_and_save_history(name, history, model, save_path, subfolder='CNN')
del name
\end{lstlisting}

\hypertarget{cnn-dct}{%
\subsubsection*{CNN-DCT}\label{cnn-dct}}

\begin{lstlisting}[language=Python]
from keras.layers import BatchNormalization, Dropout, Conv1D, Dense
name = 'CNN_32F_3_5_16F_17_27_47_DCT_8F_3_5_7_21'
subfolder = 'DCT'
filter_size = 32

inputs = keras.Input(shape=(1,1024))

x_1 = BatchNormalization()(inputs)
x_1 = Conv1D(filters=n_filters, kernel_size=3,  activation='leaky_relu', data_format='channels_first')(x_1)
x_1 = BatchNormalization()(x_1)
x_1 = Conv1D(filters=n_filters, kernel_size=7,  activation='leaky_relu', data_format='channels_first')(x_1)
x_1 = BatchNormalization()(x_1)
x_1 = layers.MaxPooling1D(2)(x_1)

x_1 = Conv1D(filters=n_filters/2, kernel_size=17, activation='leaky_relu', data_format='channels_first')(x_1)
x_1 = BatchNormalization()(x_1)
x_1 = Conv1D(filters=n_filters/2, kernel_size=27, activation='leaky_relu', data_format='channels_first')(x_1)
x_1 = BatchNormalization()(x_1)
x_1 = Conv1D(filters=n_filters/2, kernel_size=47, activation='leaky_relu', data_format='channels_first')(x_1)
x_1 = layers.MaxPooling1D(2)(x_1)
x_1 = layers.Flatten()(x_1)

x_2 = tf.signal.dct(inputs, name='dct_transform')
x_2 = BatchNormalization()(x_2)
x_2 = Conv1D(filters=filter_size/4, kernel_size=3,activation='leaky_relu', data_format='channels_first')(x_2)
x_2 = BatchNormalization()(x_2)
x_2 = Conv1D(filters=filter_size/4, kernel_size=5,activation='leaky_relu', data_format='channels_first')(x_2)
x_2 = BatchNormalization()(x_2)
x_2 = Conv1D(filters=filter_size/4, kernel_size=7,activation='leaky_relu', data_format='channels_first')(x_2)
x_2 = BatchNormalization()(x_2)
x_2 = Conv1D(filters=filter_size/4, kernel_size=21,activation='leaky_relu',data_format='channels_first')(x_2)
x_2 = layers.MaxPooling1D(2)(x_2)
x_2 = layers.Flatten(name='dct_features')(x_2)


x_3 = layers.Concatenate()([x_1, x_2])

x_3 = Dropout(0.2)(x_3)
x_3 = layers.Dense(1024, activation='leaky_relu')(x_3)
x_3 = Dropout(0.2)(x_3)
x_3 = layers.Dense(512, activation='leaky_relu', name='elements')(x_3)



output_elements = layers.Dense(n_elements, activation='leaky_relu')(x_3)
output_elements = layers.Softmax(axis=-1)(output_elements)
output_elements = layers.Reshape((1, n_elements), name='output1')(output_elements)

model = keras.Model(inputs=inputs, outputs=output_elements, name="cnn_dct")
\end{lstlisting}

\begin{lstlisting}[language=Python]
batch_size = 1024

callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', 
                                            patience=7, 
                                            min_delta=0.01,
                                            restore_best_weights=True)

model.compile(
    optimizer= keras.optimizers.Adam(learning_rate=0.00001),
    loss= tf.keras.losses.CategoricalCrossentropy(),
    metrics = 'categorical_accuracy')

device = cuda.get_current_device()

history = model.fit(
    x_train.reshape(x_train.shape[0], 1, 1024),
    y_train,
    batch_size = batch_size,
    verbose = 1,
    epochs = 200,
    shuffle=True,
    callbacks=[callback],
    validation_data = (
                        x_test.reshape(x_test.shape[0], 1, 1024),
                        y_test
                       ),
    )

gc.collect()
functions_tf.plot_and_save_history(name, history, model, save_path, subfolder=subfolder)
del name
\end{lstlisting}

\hypertarget{cbam}{%
\subsection*{CBAM}\label{cbam}}

\begin{lstlisting}[language=Python]
from functions_tf import build_1d_resnet_with_cbam

input_shape = (1, 1024)  # Adapted input shape
num_filters = 1024 # Increase the number of filters in the CBAM block
model = build_1d_resnet_with_cbam(input_shape=input_shape, num_classes=n_elements, num_filters=num_filters, output_shape=(1, n_elements), res_block_num=1)
\end{lstlisting}


\begin{lstlisting}[language=Python]
name = 'CBAM_2Blocks_512F_bs2048'
subfolder = 'CBAM'
callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', 
                                            patience=15,
                                            min_delta=0.0001,
                                            restore_best_weights=True)

model.compile(optimizer= keras.optimizers.Adam(learning_rate=0.000001),
loss= tf.keras.losses.CategoricalCrossentropy(),
metrics = 'categorical_accuracy')


history = model.fit(
    x_train.reshape(x_train.shape[0], 1, 1024),
    y_train,
    batch_size = 2048,
    verbose = 1,
    epochs = 250,
    shuffle=True,
    callbacks=[callback],
    validation_data = (x_test.reshape(x_test.shape[0], 1, 1024), y_test)
    )


gc.collect()
keras.utils.plot_model(model, f'{name}.png', show_layer_names=True, show_layer_activations=True, show_shapes=True)
functions_tf.plot_and_save_history(name, history, model, save_path, subfolder=subfolder)
del name
\end{lstlisting}

\hypertarget{vision-transformer-model-vit}{%
\subsection*{Vision transformer model
(ViT)}\label{vision-transformer-model-vit}}

\begin{lstlisting}[language=Python]
from functions_tf import VisionTransformer

vit = VisionTransformer(
    patch_size=32,
    hidden_size=1024,
    depth=8,
    num_heads=6,
    mlp_dim=128,
    num_classes=n_elements,
    sd_survival_probability=1,
    dropout=0.1,
    attention_dropout=0.1
)
vit.build((None, 1024, 1))
\end{lstlisting}

\begin{lstlisting}[language=Python]
vit.compile(optimizer= keras.optimizers.Adam(learning_rate=0.0001),
            loss= tf.keras.losses.CategoricalCrossentropy(),
            metrics = 'categorical_accuracy')

callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10,
                                            min_delta=0.001,
                                            restore_best_weights=True)

history = vit.fit(
    x_train.reshape(x_train.shape[0],1024,1),
    tf.reshape(y_train, (y_train.shape[0], n_elements)),
    batch_size = 512,
    verbose = 1,
    epochs = 200,
    validation_data = (x_test.reshape(x_test.shape[0], 1024,1),
                       tf.reshape(y_test, (y_test.shape[0], n_elements))),
    callbacks=[callback]
    )
# save model
name = 'vit_32_1024_8_6_128'
vit.save_weights(f'{save_path}\\{name}_weights.h5')
# save history
pickle.dump(history.history, open(f'{save_path}\\plots_data\\history_{name}.pkl', 'wb'))
del name
\end{lstlisting}